{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Wikipedia files into database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This files contains code to process the files from wikipedia, clean them and load them into the databse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from tempfile import mkstemp\n",
    "from shutil import move\n",
    "from os import fdopen, remove\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sqlalchemy as sql\n",
    "import csv\n",
    "from tqdm import tqdm_notebook\n",
    "import wikipedia\n",
    "from multiprocessing import Pool\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"/home/justina/Desktop/dv/data_viz/server/wiki-data/2016-2018\"\n",
    "lst_data= os.listdir(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to read teh file from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace(file_path):\n",
    "    fh, abs_path = mkstemp()\n",
    "    with fdopen(fh, 'w') as new_file:\n",
    "        with open(file_path) as old_file:\n",
    "            for line in old_file:\n",
    "                head = line.split(';')[0:-2]\n",
    "                tail= line.split(';')[-2:]\n",
    "                tail = ' '.join(tail)\n",
    "                head = ';'.join(head)\n",
    "                final = head + ' ' + tail\n",
    "                new_file.write(final)\n",
    "                \n",
    "    old = len(open(file_path).readlines())\n",
    "    new = len(open(abs_path).readlines())\n",
    "    \n",
    "    print(\"Number of lines: \")\n",
    "    print(\"{}, Old: {}, New: {}\".format(file_path,old, new))\n",
    "    \n",
    "    if old == new:\n",
    "        move(abs_path, file_path)\n",
    "        \n",
    "for file in lst_data:\n",
    "    if 'pagecounts' in file:\n",
    "        if 'spaces' not in file:\n",
    "            replace(os.path.join(DIR, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data file, we have to sort them and get only the top 1000, so that we focus only on the most viewed articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(folder_of_files):\n",
    "    # df of article name, monthly viewcount, day\n",
    "    \n",
    "    lst_data = os.listdir(folder_of_files)\n",
    "    # lst_data=['pagecounts-2016-10-views-ge-5_cleaned']\n",
    "    columns = ['article_name', \n",
    "               'monthly_viewcount', \n",
    "               'day']\n",
    "    \n",
    "    df = pd.DataFrame([])\n",
    "    year_month = []\n",
    "    for file in lst_data:\n",
    "        if 'pagecounts' in file:\n",
    "            next_df = pd.read_csv(os.path.join(folder_of_files, file), \\\n",
    "                                  sep=' ', \\\n",
    "                                  dtype={columns[0]:str, \\\n",
    "                                         columns[1]:int, columns[2]:int},\\\n",
    "                                  header=None, names=columns)\n",
    "            \n",
    "            lst_filename = file.split('-')\n",
    "            yr = lst_filename[1]\n",
    "            mth = lst_filename[2]\n",
    "            next_df['year'] = yr\n",
    "            next_df['month'] = mth\n",
    "            year_month.append((int(yr), int(mth)))\n",
    "            \n",
    "            # sort them by most viewed\n",
    "            next_df.sort_values(by='monthly_viewcount', ascending=False, inplace=True)\n",
    "            print(\"Before filtering {}-{}: {}\".format(yr,mth,next_df.year.size))\n",
    "            \n",
    "            # Filter unwanted data out\n",
    "            mask = next_df.article_name.str.contains('Special:') | \\\n",
    "                    next_df.article_name.str.contains('Main_Page') | \\\n",
    "                    next_df.article_name.str.contains('Portal:') | \\\n",
    "                    next_df.article_name.str.contains('Wikipedia:') |\\\n",
    "                    next_df.article_name.str.contains('List of ') |\\\n",
    "                    next_df.article_name.str.contains('Special%:') |\\\n",
    "                    next_df.article_name.str.contains('User:') |\\\n",
    "                    next_df.article_name.str.contains('Help:') |\\\n",
    "                    next_df.article_name.str.contains('Category:') |\\\n",
    "                    next_df.article_name.str.contains('-') |\\\n",
    "                    next_df.article_name.str.contains('404.php') |\\\n",
    "                    next_df.article_name.str.contains('File:')\n",
    "            \n",
    "            next_df = next_df[~mask]\n",
    "            print(\"After filtering: {}\".format(next_df.year.size))\n",
    "            \n",
    "            # add them to our main data frame\n",
    "            df = pd.concat([df, next_df.head(1000)], ignore_index = True)\n",
    "    \n",
    "    df = df.astype({'article_name': 'str', 'monthly_viewcount': 'int64', \n",
    "                    'day': 'int64', 'year': 'int64', 'month':'int64'})\n",
    "    \n",
    "    df['peak_date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe whichi contains all data we're interested in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_df(os.path.join(DIR, 'final'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['year', 'month']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikipedia.set_lang('en')\n",
    "wikipedia.set_rate_limiting(rate_limit = True)\n",
    "\n",
    "def get_article_data(x):\n",
    "    row = x\n",
    "    try:\n",
    "        name = row['article_name']\n",
    "        page = wikipedia.WikipediaPage(title=name)\n",
    "        return name, page, row\n",
    "        \n",
    "    except:\n",
    "        name = row['article_name']\n",
    "        print(name)\n",
    "        return name, None, row\n",
    "\n",
    "def insert(lst, con, articles, article_name, \\ \n",
    "           year, month, day, view_count, peak_date, \\\n",
    "           summary, page_id):\n",
    "    \n",
    "    lst.append({'title': article_name,\n",
    "                   'year': year,\n",
    "                   'month':month,\n",
    "                   'day':day,\n",
    "                   'view_count':view_count,\n",
    "                   'peak_date': peak_date,\n",
    "                   'summary': summary,\n",
    "                   'page_id': page_id})\n",
    "    \n",
    "    if len(lst) > 50:\n",
    "        print(\"Inserting...\")\n",
    "        con.execute(articles.insert(), lst)\n",
    "        lst = []\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromdb_df = pd.read_csv('./2016-2018/top_1000_2016-2018/fromdb_data.csv', \\\n",
    "                        names = ['article_name', 'summary', 'page_id'])\n",
    "fromdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_final_data(filepath):    \n",
    "    # df has title, year, month, day, viewcount, peak date\n",
    "    fromdb_df = pd.read_csv('./2016-2018/top_1000_2016-2018/fromdb_data.csv', \\\n",
    "                            names = ['article_name', 'summary', 'page_id'], na_values = '')\n",
    "    \n",
    "    db_dict = {}\n",
    "    for i, row in fromdb_df.iterrows():\n",
    "        db_dict[row['article_name']] = (row['summary'], row['page_id'])\n",
    "        \n",
    "    article_names = set(fromdb_df.article_name.values)\n",
    "    new_df = pd.DataFrame(columns = ['title', 'year', 'month', 'day', \\\n",
    "                                     'view_count', 'peak_date', 'summary', 'page_id'])\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    exists = []\n",
    "    \n",
    "    for _, row in tqdm_notebook(df.iterrows(), total = len(df)):\n",
    "        try:\n",
    "            title = row['article_name']\n",
    "            year = row['year']\n",
    "            month = row['month']\n",
    "            day = row['day']\n",
    "            view_count =row['monthly_viewcount']\n",
    "            peak_date = row['peak_date']\n",
    "\n",
    "            if (title in db_dict):\n",
    "                summary = db_dict[title][0]\n",
    "                page_id = db_dict[title][1]\n",
    "            else:\n",
    "                _, page, _ = get_article_data(row)\n",
    "                if page:\n",
    "                    summary = page.summary\n",
    "                    page_id = page.pageid\n",
    "                    \n",
    "                else:\n",
    "                    summary = \"\"\n",
    "                    page_id = -1\n",
    "                db_dict[title] = (summary, page_id)\n",
    "            \n",
    "            new_df = new_df.append({'title': title,\n",
    "                          'year': year,\n",
    "                          'month': month,\n",
    "                          'day': day,\n",
    "                          'view_count': view_count,\n",
    "                          'peak_date': peak_date,\n",
    "                          'summary': summary,\n",
    "                          'page_id': page_id}, ignore_index= True)\n",
    "            \n",
    "        except Exception as e: \n",
    "            # In case of failure we don't want the whole processing to stop\n",
    "            print(\"Failed\", row['article_name'])\n",
    "            print(e)\n",
    "\n",
    "    new_df.to_csv('./2016-2018/top_1000_2016-2018/final_data_0.csv')\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_final_data('./2016-2018/top_1000_2016-2018/partial_view_data0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./wiki-data/2016-2018/top_1000_2016-2018/view_data.csv')\n",
    "df_array = np.array_split(df,2)\n",
    "for i, dframe in enumerate(df_array):\n",
    "    dframe.to_csv('./wiki-data/2016-2018/top_1000_2016-2018/partial_view_data{}.csv'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add may 2017 data to db again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP1000_DIR = \"/home/justina/Desktop/dv/data_viz/server/wiki-data/top_1000_per_month\"\n",
    "\n",
    "df1 = pd.DataFrame([])\n",
    "for f in os.listdir(TOP1000_DIR):\n",
    "    if 'counts' in f:\n",
    "        df1 = pd.concat([df1, pd.read_csv(os.path.join(TOP1000_DIR, f))], ignore_index = True)\n",
    "        \n",
    "df1.year.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[(df1.month == 5) & (df1.year == 2017)].head(50)[['article_name', 'peak_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entries(engine_path, df):\n",
    "    engine = sql.create_engine(engine_path)\n",
    "    metadata = sql.MetaData()\n",
    "    articles = sql.Table('articles', metadata,\n",
    "        sql.Column('title', sql.String),\n",
    "        sql.Column('year', sql.Integer),\n",
    "        sql.Column('month', sql.Integer),\n",
    "        sql.Column('day', sql.Integer),\n",
    "        sql.Column('view_count', sql.Integer), \n",
    "        sql.Column('peak_date', sql.Date),\n",
    "        sql.Column('summary', sql.Text),\n",
    "        sql.Column('page_id', sql.Integer)\n",
    "    )\n",
    "    metadata.create_all(engine)\n",
    "\n",
    "    result = []\n",
    "    errors = []\n",
    "    wiki_df = pd.DataFrame(columns=['article_name', 'summary', 'page_id'])\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        lst = list(df.iterrows())\n",
    "        for _, row in lst:\n",
    "            name = row['article_name']\n",
    "            year = row['year']\n",
    "            month = row['month']\n",
    "            day = row['day']\n",
    "            view_count = row['monthly_viewcount']\n",
    "            peak_date = datetime.datetime.strptime(row['peak_date'], '%Y-%m-%d').date()\n",
    "\n",
    "            if name not in wiki_df.article_name.values:\n",
    "                name, page, r = get_article_data(row)\n",
    "\n",
    "                if page:\n",
    "                    wiki_df = wiki_df.append({'article_name': name, 'summary': page.summary, 'page_id': page.pageid}, ignore_index = True)\n",
    "                else:\n",
    "                    wiki_df = wiki_df.append({'article_name': name, 'summary': \"\", 'page_id': -1}, ignore_index = True)\n",
    "\n",
    "                result = insert(result,con, articles, name, year, month, day, view_count, peak_date, wiki_df[wiki_df['article_name'] == name].summary.values[0], \\\n",
    "                                wiki_df[wiki_df['article_name'] == name].page_id.values[0])  \n",
    "\n",
    "        con.execute(articles.insert(), result)\n",
    "        return wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load file into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['title', 'year', 'month', 'day', 'view_count', 'peak_date', 'summary', 'page_id']\n",
    "\n",
    "df_input0 = pd.read_csv('./2016-2018/top_1000_2016-2018/final_data_0.csv', names = columns, skiprows=1)\n",
    "df_input1 = pd.read_csv('./2016-2018/top_1000_2016-2018/final_data_1.csv', names = columns, skiprows=1)\n",
    "df_all = pd.concat([df_input0, df_input1], ignore_index=True)\n",
    "df_all = df_all.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_all.page_id ==-1)\n",
    "df_all = df_all[~mask].iloc[:,1:]\n",
    "print(\"Remove where page id is -1 \", len(df_all))\n",
    "\n",
    "df_all = df_all[~df_all.title.str.contains(\"Template:\")]\n",
    "print(\"Remove articles with Template: \", len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sql.create_engine('sqlite:///kiru_db.db')\n",
    "metadata = sql.MetaData()\n",
    "articles = sql.Table('articles', metadata,\n",
    "                sql.Column('title', sql.String),\n",
    "                sql.Column('year', sql.Integer),\n",
    "                sql.Column('month', sql.Integer),\n",
    "                sql.Column('day', sql.Integer),\n",
    "                sql.Column('view_count', sql.Integer),\n",
    "                sql.Column('peak_date', sql.Date),\n",
    "                sql.Column('summary', sql.Text),\n",
    "                sql.Column('page_id', sql.Integer)\n",
    "                )\n",
    "metadata.create_all(engine)\n",
    "\n",
    "\n",
    "with engine.connect() as con:\n",
    "\n",
    "    for row in tqdm_notebook(df_input.iterrows(), total=len(df_input)):\n",
    "        row = row[1]\n",
    "        title = row['title']\n",
    "        year = row['year']\n",
    "        month = row['month']\n",
    "        day = row['day']\n",
    "        view_count =row['view_count']\n",
    "        peak_date = row['peak_date']\n",
    "        summary = row['summary']\n",
    "        page_id = row['page_id']\n",
    "\n",
    "        peak_date = datetime.datetime.strptime(peak_date, '%Y-%m-%d').date()\n",
    "        con.execute(articles.insert(), {'title': title,\n",
    "                                        'year': year,\n",
    "                                        'month': month,\n",
    "                                        'day': day,\n",
    "                                        'view_count': view_count,\n",
    "                                        'peak_date': peak_date,\n",
    "                                        'summary': summary,\n",
    "                                        'page_id': page_id})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
