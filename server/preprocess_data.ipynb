{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "1. For each month, sort articles by viewcount\n",
    "2. Limit to top 1000 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from tempfile import mkstemp\n",
    "from shutil import move\n",
    "from os import fdopen, remove\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sqlalchemy as sql\n",
    "import csv\n",
    "from tqdm import tqdm_notebook\n",
    "import wikipedia\n",
    "from multiprocessing import Pool\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"/home/justina/Desktop/dv/data_viz/server/wiki-data/2016-2018\"\n",
    "lst_data= os.listdir(DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace(file_path):\n",
    "\n",
    "    fh, abs_path = mkstemp()\n",
    "    with fdopen(fh, 'w') as new_file:\n",
    "        with open(file_path) as old_file:\n",
    "            for line in old_file:\n",
    "                head = line.split(';')[0:-2]\n",
    "                tail= line.split(';')[-2:]\n",
    "                tail = ' '.join(tail)\n",
    "                head = ';'.join(head)\n",
    "                final = head + ' ' + tail\n",
    "                new_file.write(final)\n",
    "    old = len(open(file_path).readlines())\n",
    "    new = len(open(abs_path).readlines())\n",
    "    print(\"Number of lines: \")\n",
    "    print(\"{}, Old: {}, New: {}\".format(file_path,old, new))\n",
    "    \n",
    "    if old == new:\n",
    "        move(abs_path, file_path)\n",
    "        \n",
    "for file in lst_data:\n",
    "    if 'pagecounts' in file:\n",
    "        if 'spaces' not in file:\n",
    "            replace(os.path.join(DIR, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(folder_of_files):\n",
    "    # df of article name, monthly viewcount, day\n",
    "    \n",
    "    lst_data = os.listdir(folder_of_files)\n",
    "    # lst_data=['pagecounts-2016-10-views-ge-5_cleaned']\n",
    "    columns = ['article_name', \n",
    "               'monthly_viewcount', \n",
    "               'day']\n",
    "    df = pd.DataFrame([])\n",
    "    year_month = []\n",
    "    for file in lst_data:\n",
    "        if 'pagecounts' in file:\n",
    "            next_df = pd.read_csv(os.path.join(folder_of_files, file), sep=' ',dtype={columns[0]:str, columns[1]:int, columns[2]:int}, header=None, names=columns)\n",
    "            lst_filename = file.split('-')\n",
    "            yr = lst_filename[1]\n",
    "            mth = lst_filename[2]\n",
    "            next_df['year'] = yr\n",
    "            next_df['month'] = mth\n",
    "            year_month.append((int(yr), int(mth)))\n",
    "            next_df.sort_values(by='monthly_viewcount', ascending=False, inplace=True)\n",
    "            print(\"Before filtering {}-{}: {}\".format(yr,mth,next_df.year.size))\n",
    "            mask = next_df.article_name.str.contains('Special:') | \\\n",
    "                    next_df.article_name.str.contains('Main_Page') | \\\n",
    "                    next_df.article_name.str.contains('Portal:') | \\\n",
    "                    next_df.article_name.str.contains('Wikipedia:') |\\\n",
    "                    next_df.article_name.str.contains('List of ') |\\\n",
    "                    next_df.article_name.str.contains('Special%:') |\\\n",
    "                    next_df.article_name.str.contains('User:') |\\\n",
    "                    next_df.article_name.str.contains('Help:') |\\\n",
    "                    next_df.article_name.str.contains('Category:') |\\\n",
    "                    next_df.article_name.str.contains('-') |\\\n",
    "                    next_df.article_name.str.contains('404.php') |\\\n",
    "                    next_df.article_name.str.contains('File:')\n",
    "            \n",
    "            next_df=next_df[~mask]\n",
    "            print(\"After filtering: {}\".format(next_df.year.size))\n",
    "            df = pd.concat([df, next_df.head(1000)], ignore_index = True)\n",
    "    \n",
    "    df = df.astype({'article_name': 'str', 'monthly_viewcount': 'int64', 'day': 'int64', 'year': 'int64', 'month':'int64'})\n",
    "    df['peak_date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = df.astype({'article_name': 'str', 'monthly_viewcount': 'int64', 'day': 'int64', 'year': 'int64', 'month':'int64'})\n",
    "# df['peak_date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "# df.head()\n",
    "\n",
    "df = create_df(os.path.join(DIR, 'final'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['year', 'month']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikipedia.set_lang('en')\n",
    "wikipedia.set_rate_limiting(rate_limit = True)\n",
    "def get_article_data(x):\n",
    "    row = x\n",
    "    try:\n",
    "        name = row['article_name']\n",
    "        page = wikipedia.WikipediaPage(title=name)\n",
    "        return name, page, row\n",
    "        \n",
    "    except:\n",
    "        name = row['article_name']\n",
    "        print(name)\n",
    "        return name, None, row\n",
    "\n",
    "def insert(lst, con, articles, article_name, year, month, day, view_count, peak_date, summary, page_id):\n",
    "    lst.append({'title': article_name,\n",
    "                   'year': year,\n",
    "                   'month':month,\n",
    "                   'day':day,\n",
    "                   'view_count':view_count,\n",
    "                   'peak_date': peak_date,\n",
    "                   'summary': summary,\n",
    "                   'page_id': page_id})\n",
    "    if len(lst) > 50:\n",
    "        print(\"Inserting...\")\n",
    "        con.execute(articles.insert(), lst)\n",
    "#         except:\n",
    "#             print(\"Did not insert\")\n",
    "        lst = []\n",
    "    return lst\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromdb_df = pd.read_csv('./2016-2018/top_1000_2016-2018/fromdb_data.csv', names = ['article_name', 'summary', 'page_id'])\n",
    "fromdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'404.php' in fromdb_df.article_name\n",
    "fromdb_df.loc[1].article_name\n",
    "\n",
    "article_names = set(fromdb_df.article_name.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'404.php' in article_names\n",
    "# summary = fromdb_df[fromdb_df['article_name']=='adsf'].summary.values[0]\n",
    "print(fromdb_df[fromdb_df['article_name']=='404.php'].summary.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_final_data(filepath):    \n",
    "    # df has title, year, month, day, viewcount, peak date\n",
    "    fromdb_df = pd.read_csv('./2016-2018/top_1000_2016-2018/fromdb_data.csv', names = ['article_name', 'summary', 'page_id'], na_values = '')\n",
    "    db_dict = {}\n",
    "    for i, row in fromdb_df.iterrows():\n",
    "        db_dict[row['article_name']] = (row['summary'], row['page_id'])\n",
    "        \n",
    "    article_names = set(fromdb_df.article_name.values)\n",
    "    new_df = pd.DataFrame(columns = ['title', 'year', 'month', 'day', 'view_count', 'peak_date', 'summary', 'page_id'])\n",
    "    df = pd.read_csv(filepath)\n",
    "    exists = []\n",
    "    for _, row in tqdm_notebook(df.iterrows(), total = len(df)):\n",
    "        try:\n",
    "            title = row['article_name']\n",
    "            year = row['year']\n",
    "            month = row['month']\n",
    "            day = row['day']\n",
    "            view_count =row['monthly_viewcount']\n",
    "            peak_date = row['peak_date']\n",
    "\n",
    "            if (title in db_dict):\n",
    "                summary = db_dict[title][0]\n",
    "                page_id = db_dict[title][1]\n",
    "            else:\n",
    "                _, page, _ = get_article_data(row)\n",
    "                if page:\n",
    "                    summary = page.summary\n",
    "                    page_id = page.pageid\n",
    "                    \n",
    "                else:\n",
    "                    summary = \"\"\n",
    "                    page_id = -1\n",
    "                db_dict[title] = (summary, page_id)\n",
    "            new_df = new_df.append({'title': title,\n",
    "                          'year': year,\n",
    "                          'month': month,\n",
    "                          'day': day,\n",
    "                          'view_count': view_count,\n",
    "                          'peak_date': peak_date,\n",
    "                          'summary': summary,\n",
    "                          'page_id': page_id}, ignore_index= True)\n",
    "        except Exception as e: \n",
    "            print(\"Failed\", row['article_name'])\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    new_df.to_csv('./2016-2018/top_1000_2016-2018/final_data_0.csv')\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daff6b20792f40f7a4eaf0fa829baaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters_of_Warcraft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justina/Desktop/dv/dv/lib/python3.5/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/justina/Desktop/dv/dv/lib/python3.5/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice_Through_the_Looking_Glass_(film)\n"
     ]
    }
   ],
   "source": [
    "get_final_data('./2016-2018/top_1000_2016-2018/partial_view_data0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df.article_name.unique()) - set(fromdb_df.article_name.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./wiki-data/2016-2018/top_1000_2016-2018/view_data.csv')\n",
    "df_array = np.array_split(df,2)\n",
    "for i, dframe in enumerate(df_array):\n",
    "    dframe.to_csv('./wiki-data/2016-2018/top_1000_2016-2018/partial_view_data{}.csv'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from top 1000 per month\n",
    "TOP1000_DIR = \"/home/justina/Desktop/dv/data_viz/server/wiki-data/top_1000_2016-2018\"\n",
    "\n",
    "df1 = pd.DataFrame([])\n",
    "for f in os.listdir(TOP1000_DIR):\n",
    "    if 'counts' in f:\n",
    "        df1 = pd.concat([df1, pd.read_csv(os.path.join(TOP1000_DIR, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.year.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rows(limit=10):\n",
    "    i = 0\n",
    "    with engine.connect() as con:\n",
    "        rows = con.execute(articles.select())\n",
    "        for r in rows:\n",
    "            print(r.title, int(r.view_count))\n",
    "            i += 1\n",
    "            if ( i == limit): return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_folder_path = \"/home/justina/Desktop/dv/data_viz/server/wiki/\"\n",
    "\n",
    "links = open(os.path.join(urls_folder_path, 'urls_all.txt'),'r').readlines()\n",
    "for i in range(0, len(links),5):\n",
    "    fh, abs_path = mkstemp()\n",
    "    with fdopen(fh, 'w') as new_file:\n",
    "        print(i, i+5)\n",
    "        for j in links[i:i+5]:\n",
    "            new_file.write(j)\n",
    "    move(abs_path, os.path.join(urls_folder_path, 'url{}.txt'.format(i)))\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add may 2017 data to db again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP1000_DIR = \"/home/justina/Desktop/dv/data_viz/server/wiki-data/top_1000_per_month\"\n",
    "\n",
    "df1 = pd.DataFrame([])\n",
    "for f in os.listdir(TOP1000_DIR):\n",
    "    if 'counts' in f:\n",
    "        df1 = pd.concat([df1, pd.read_csv(os.path.join(TOP1000_DIR, f))], ignore_index = True)\n",
    "        \n",
    "df1.year.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[(df1.month == 5) & (df1.year == 2017)].head(50)[['article_name', 'peak_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entries(engine_path, df):\n",
    "    engine = sql.create_engine(engine_path)\n",
    "    metadata = sql.MetaData()\n",
    "    articles = sql.Table('articles', metadata,\n",
    "        sql.Column('title', sql.String),\n",
    "        sql.Column('year', sql.Integer),\n",
    "        sql.Column('month', sql.Integer),\n",
    "        sql.Column('day', sql.Integer),\n",
    "        sql.Column('view_count', sql.Integer), \n",
    "        sql.Column('peak_date', sql.Date),\n",
    "        sql.Column('summary', sql.Text),\n",
    "        sql.Column('page_id', sql.Integer)\n",
    "    )\n",
    "    metadata.create_all(engine)\n",
    "\n",
    "    result = []\n",
    "    errors = []\n",
    "    wiki_df = pd.DataFrame(columns=['article_name', 'summary', 'page_id'])\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        lst = list(df.iterrows())\n",
    "    #     with Pool(50) as pool:\n",
    "    #     for each in tqdm_notebook(pool.imap(get_article_data, lst), total=len(lst)):\n",
    "        for _, row in lst:\n",
    "            name = row['article_name']\n",
    "            year = row['year']\n",
    "            month = row['month']\n",
    "            day = row['day']\n",
    "            view_count = row['monthly_viewcount']\n",
    "            peak_date = datetime.datetime.strptime(row['peak_date'], '%Y-%m-%d').date()\n",
    "\n",
    "            if name not in wiki_df.article_name.values:\n",
    "                name, page, r = get_article_data(row)\n",
    "\n",
    "                if page:\n",
    "                    wiki_df = wiki_df.append({'article_name': name, 'summary': page.summary, 'page_id': page.pageid}, ignore_index = True)\n",
    "                else:\n",
    "                    wiki_df = wiki_df.append({'article_name': name, 'summary': \"\", 'page_id': -1}, ignore_index = True)\n",
    "\n",
    "                result = insert(result,con, articles, name, year, month, day, view_count, peak_date, wiki_df[wiki_df['article_name'] == name].summary.values[0], wiki_df[wiki_df['article_name'] == name].page_id.values[0])\n",
    "            \n",
    "\n",
    "        con.execute(articles.insert(), result)\n",
    "                \n",
    "        return wiki_df\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df=add_entries('sqlite:///test.db', df1.loc[100:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sql.create_engine('sqlite:///test.db')\n",
    "metadata = sql.MetaData()\n",
    "articles = sql.Table('articles', metadata,\n",
    "    sql.Column('title', sql.String),\n",
    "    sql.Column('year', sql.Integer),\n",
    "    sql.Column('month', sql.Integer),\n",
    "    sql.Column('day', sql.Integer),\n",
    "    sql.Column('view_count', sql.Integer), \n",
    "    sql.Column('peak_date', sql.Date),\n",
    "    sql.Column('summary', sql.Text),\n",
    "    sql.Column('page_id', sql.Integer)\n",
    ")\n",
    "metadata.create_all(engine)\n",
    "\n",
    "result = []\n",
    "errors = []\n",
    "wiki_df = pd.DataFrame(columns=['article_name', 'summary', 'page_id'])\n",
    "\n",
    "with engine.connect() as con:\n",
    "    name = df1.loc[0,'article_name']\n",
    "    year = df1.loc[0,'year']\n",
    "    month = df1.loc[0,'month']\n",
    "    day = df1.loc[0,'day']\n",
    "    view_count = df1.loc[0,'monthly_viewcount']\n",
    "    peak_date = datetime.datetime.strptime(df1.loc[0,'peak_date'], '%Y-%m-%d').date()\n",
    "\n",
    "    name, page, row = get_article_data((df1.loc[0]))\n",
    "    con.execute(articles.insert(), {'title': name,\n",
    "                                   'year': year,\n",
    "                                   'month': month,\n",
    "                                   'day': day,\n",
    "                                   'view_count': view_count,\n",
    "                                   'peak_date': peak_date,\n",
    "                                   'summary': \"\",\n",
    "                                   'page_id': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[0,'peak_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = add_entries('sqlite:///database_working_add_052017.db',df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = add_entries('sqlite:///database_working_add_052017.db',df1[851:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1.article_name == 'Restoration_(Spain)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
